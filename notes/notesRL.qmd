---
title: "Notes on RL for Option"
format: 
  html:
    toc: true
    code-fold: true 
    linenumbers: false
    doublespacing: false
keywords: 
  - template
  - demo
#bibliography: bibliography.bib  
---

# Introduction {#sec-intro}

## MDP model for American Option Pricing

- **State** is [Current Time, Relevant History of Underlying Security Prices]
- **Action** is Boolean: Exercise(i.e, Stop) or Continue
- **Reward** is always 0, except upon Exercise (When the Reward is euqal to the Payoff)
- State-transitions are based on the Underlying Securities Risk-Neutral Process.


$$
Q(s,a; \mathbf{w})=\begin{cases}
    \phi(s)^T.\mathbf{w},& \text{if  } a=c  \\ 
    g(s)              & \text{if  } a=e
\end{cases}
$$


Feature functions $\phi(.)= [\phi(.)|i=1,\cdots, m]$

### LSPI Semi-Gradient Equation

$$\sum_i\phi(s_i).(\phi(s_i)^T. \mathbf{w}^*-\gamma.Q(s_i^{\prime}, \pi_D(s_i^{\prime});\mathbf{w}^*)$$

- $$Q(s_i^{\prime}, \pi_D(s_i^{\prime});\mathbf{w}^*)$$

- $C1$: if $s_i^{\prime}$ is non-terminal and $\pi_D(s^{\prime}) = c(i.e, \phi(s_i^{\prime}).\mathbf{w}>g(s_i^{\prime}))$ substitute $\phi(s_i^{\prime})^T. \mathbf{w}^*$

- $C2$: if $s_i^{\prime}$ is a terminal state or $\pi_{D}(s_{i}^{\prime})=e$ (i.e, 
$g(s_i^{\prime})>\phi(s_i^{\prime}).\mathbf{w})$: Substitute $g(s_i^{\prime})$


$$\sum_i\phi(s_i).(\phi(s_i)^T.\mathbf{w}^*- I_{C1}.\gamma.\phi(s_i^{\prime})^T.\mathbf{w}^*-I_{C2}.\gamma.g(s_{i}^{\prime}))$$

Factoring out $\mathbf{w}^{*}$ we get:
$$\sum_i\phi(s_i).(\phi(s_i)- I_{C1}.\gamma.\phi(s_i^{\prime})^T).\mathbf{w}^*=\sum_{i} I_{C2}.\phi(s_i).g(s_{i}^{\prime}))$$

This can be written in 
$$\mathbf{A}.\mathbf{w}^*=b$$

- $$\mathbf{A}=\sum_i\phi(s_i).(\phi(s_i)- I_{C1}.\gamma.\phi(s_i^{\prime})^T)$$
- $$\mathbf{b}=\sum_{i} I_{C2}.\phi(s_i).g(s_{i}^{\prime}))$$

The $m\times m$ Matrix $\mathbf{A}$ is accumulated at each atomic experince $(s_i, s_i^{\prime})$:

- $$\mathbf{A} \Leftarrow \mathbf{A} + \phi(s_i).(\phi(s_i)- I_{C1}.\gamma.\phi(s_i^{\prime})^T)$$

- $$\mathbf{b} \Leftarrow \mathbf{b} + \gamma I_{C2}.\phi(s_i).g(s_{i}^{\prime})$$

This solved $w^{*}$ updates Q-valueFunction Approximation $Q(s,a; \mathbf{w}^*)$. This defines an updated , imrpoved deterministic policy $\pi_{D}^{\prime}$ :

$$\pi_{D}^{\prime}(s) = \operatorname*{arg max }_{a} Q(s,a; \mathbf{w}^*)$$

Using 7 feature functions, the first 4 Laguerre polyomials that are function of underlying price and 3 functions of time. 

- $\phi(S_t) = 1$
- $\phi(S_t) = e^{-\frac{M_t}{2}}$
- $\phi(S_t) = e^{-\frac{M_t}{2}}.(1-M_t)$
- $\phi(S_t) = e^{-\frac{M_t}{2}}.(1-2M_t+M_t^2/2)$$
- $\phi(S_t) = sin(\frac{\pi(T-t)}{2T})$
- $\phi(S_t) = log(T-t)$
- $\phi(S_t) = (\frac{t}{T})^2$