# Methods


## Binomial Options Pricing Model 

- The orginal Bin was developed to price option on underlying whose price evolves according 
  to a **lognormal stochastic process, where stochartic process is in form of discrete-time, finite-horizon, finite states.

- $$dS_t = r.S_t.dt + \sigma.S_t.dz_t$$

$\sigma$ is the lognormal dispersion (volatility), $r$ lognormal drift

- What is Bin : serves as a discrete-time, finite-horizon, finite-states approximation to the 
  continous proocess.

![Binomial Option Pricing Model (Binomial Tree)](figures_imprted/bin_fig.png){#fig-binomial}

- We denote $S_{i,j}$ as the price after $i$ time steps in state $j$ (for any $i$ \in Z>0 and for any $0<j<i$), 
- We need to calibrate $S_{i+1,j+1}=S_{i,j}.u$ and $S_{i+1,j}=S_{i,j}.d$
- Variance Calibration
  $$log^²(u) = \frac{\sigma^2T}{n} \Rightarrow u = e^{\sigma \sqrt{\frac{T}{n}} }$$
- Mean Calibration
- $$qu + \frac{1-q}{u} = e^{\frac{rT}{n}} \Rightarrow q = \frac{u.e^{\frac{rT}{n}-}-1}{u^2-1}=\frac{e^{\frac{rT}{n}+\sigma\sqrt{\frac{T}{n}}}-1}{e^{2\sigma\sqrt{{\frac{T}{n}}}}-1}$$
## Least Square Monte-Carlo

- The calibration for $u$ and $q$ ensures that as $n \rightarrow \infty$ (i.e., time step interval $\frac{T}{n} \rightarrow 0$), the mean and variance of binomial distributaion after i time steps matches the mean $(r-\frac{\sigma^2}{2})\frac{iT}{n}$ and variance $\frac{\sigma^2iT}{n}$ 

### Procedure

As mentioned earlier, we want to model the problem of Optimal Exercise of American Options as a discrete-time, finite-horizon, finite-states MDP. We set the terminal time to be $t = T + 1$, meaning all the states at time $T + 1$ are terminal states. Here we will utilize the states and state transitions (probabilistic price movements of the underlying) given by the Binomial Options Pricing Model as the states and state transitions in the MDP. The MDP actions in each state will be binary—either exercise the option (and immediately move to a terminal state) or don’t exercise the option (i.e., continue on to the next time step’s random state, as given by the Binomial Options Pricing Model). If the exercise action is chosen, the MDP reward is the option payoff. If the continue action is chosen, the reward is $0$. The discount factor $\lambda$ is $e^{-\frac{rT}{n}}$ since (as we’ve learnt in the single-period case), the price (which translates here to the Optimal Value Function) is defined as the riskless ratediscounted expectation (under the risk-neutral probability measure) of the option payoff. In the multi-period setting, the overall discounting amounts to composition (multiplication) of each time step’s discounting (which is equal to $\lambda$) and the overall risk-neutral probability measure amounts to the composition of each time step’s risk-neutral probability measure (which is specified by the calibrated value $q$).


## Reinforcement Learning

### MDP model for American Option Pricing

- **State** is [Current Time, Relevant History of Underlying Security Prices]
- **Action** is Boolean: Exercise(i.e, Stop) or Continue
- **Reward** is always 0, except upon Exercise (When the Reward is euqal to the Payoff)
- State-transitions are based on the Underlying Securities Risk-Neutral Process.


$$
Q(s,a; \mathbf{w})=\begin{cases}
    \phi(s)^T.\mathbf{w},& \text{if  } a=c  \\ 
    g(s)              & \text{if  } a=e
\end{cases}
$$


Feature functions $\phi(.)= [\phi(.)|i=1,\cdots, m]$

### LSPI Semi-Gradient Equation

$$\sum_i\phi(s_i).(\phi(s_i)^T. \mathbf{w}^*-\gamma.Q(s_i^{\prime}, \pi_D(s_i^{\prime});\mathbf{w}^*)$$

- $$Q(s_i^{\prime}, \pi_D(s_i^{\prime});\mathbf{w}^*)$$

- $C1$: if $s_i^{\prime}$ is non-terminal and $\pi_D(s^{\prime}) = c(i.e, \phi(s_i^{\prime}).\mathbf{w}>g(s_i^{\prime}))$ substitute $\phi(s_i^{\prime})^T. \mathbf{w}^*$

- $C2$: if $s_i^{\prime}$ is a terminal state or $\pi_{D}(s_{i}^{\prime})=e$ (i.e, 
$g(s_i^{\prime})>\phi(s_i^{\prime}).\mathbf{w})$: Substitute $g(s_i^{\prime})$


$$\sum_i\phi(s_i).(\phi(s_i)^T.\mathbf{w}^*- I_{C1}.\gamma.\phi(s_i^{\prime})^T.\mathbf{w}^*-I_{C2}.\gamma.g(s_{i}^{\prime}))$$

Factoring out $\mathbf{w}^{*}$ we get:
$$\sum_i\phi(s_i).(\phi(s_i)- I_{C1}.\gamma.\phi(s_i^{\prime})^T).\mathbf{w}^*=\sum_{i} I_{C2}.\phi(s_i).g(s_{i}^{\prime}))$$

This can be written in 
$$\mathbf{A}.\mathbf{w}^*=b$$

- $$\mathbf{A}=\sum_i\phi(s_i).(\phi(s_i)- I_{C1}.\gamma.\phi(s_i^{\prime})^T)$$
- $$\mathbf{b}=\sum_{i} I_{C2}.\phi(s_i).g(s_{i}^{\prime}))$$

The $m\times m$ Matrix $\mathbf{A}$ is accumulated at each atomic experince $(s_i, s_i^{\prime})$:

- $$\mathbf{A} \Leftarrow \mathbf{A} + \phi(s_i).(\phi(s_i)- I_{C1}.\gamma.\phi(s_i^{\prime})^T)$$

- $$\mathbf{b} \Leftarrow \mathbf{b} + \gamma I_{C2}.\phi(s_i).g(s_{i}^{\prime})$$

This solved $w^{*}$ updates Q-valueFunction Approximation $Q(s,a; \mathbf{w}^*)$. This defines an updated , imrpoved deterministic policy $\pi_{D}^{\prime}$ :

$$\pi_{D}^{\prime}(s) = \operatorname*{arg max }_{a} Q(s,a; \mathbf{w}^*)$$

Using 7 feature functions, the first 4 Laguerre polyomials that are function of underlying price and 3 functions of time. 

- $\phi(S_t) = 1$
- $\phi(S_t) = e^{-\frac{M_t}{2}}$
- $\phi(S_t) = e^{-\frac{M_t}{2}}.(1-M_t)$
- $\phi(S_t) = e^{-\frac{M_t}{2}}.(1-2M_t+M_t^2/2)$
- $\phi(S_t) = sin(\frac{\pi(T-t)}{2T})$
- $\phi(S_t) = log(T-t)$
- $\phi(S_t) = (\frac{t}{T})^2$



## LSM Method


### The Pricing is based on set of sampling traces of the underlying prices.

### Function approximation of the continuation value for in-the-money states.

The LSM approach uses least squares to approximate the conditional expectaion function at $t_{K-1}, t_{K-2}, \cdots, t_1$:

- Unknown functional form:

$$F(\omega; t_{K-1})$$

- The unknown functional form can be represented as:

$$F(\omega; t_{K-1})=\sum_{j=0}^{\infty}a_jL_j(X)$$


In this work choice of basis function is set of Lageurre polynomials:

$$L_1(X)=1$$
$$L_2(X)= 1-X$$    
$$L_3(X) = 1 - 2X + X^2/2$$
$$L_4(X) = 1 - 3X + 3X^2/2 - X^3/6$$ 
$$L_5(X) = 1 - 4X + 3X^2 - 2X^3/3 + X^4/24$$

- In the python code unknown function can be represented as:

$$F(\omega; t_{K-1})=\sum_{j=0}^{5}a_jL_j(X)$$


In this work we focus only in-the-money paths in the estimation since the exercise decision is only relevant when the option is in the money.

### Backward -recursive determination of early exercise states