[
  {
    "objectID": "methods.html#least-square-monte-carlo",
    "href": "methods.html#least-square-monte-carlo",
    "title": "2  Methods",
    "section": "2.2 Least Square Monte-Carlo",
    "text": "2.2 Least Square Monte-Carlo"
  },
  {
    "objectID": "methods.html#reinforcement-learning",
    "href": "methods.html#reinforcement-learning",
    "title": "2  Methods",
    "section": "2.2 Reinforcement Learning",
    "text": "2.2 Reinforcement Learning\n\n2.2.1 MDP model for American Option Pricing\n\nState is [Current Time, Relevant History of Underlying Security Prices]\nAction is Boolean: Exercise(i.e, Stop) or Continue\nReward is always 0, except upon Exercise (When the Reward is euqal to the Payoff)\nState-transitions are based on the Underlying Securities Risk-Neutral Process.\n\n\\[\nQ(s,a; \\mathbf{w})=\\begin{cases}\n    \\phi(s)^T.\\mathbf{w},& \\text{if  } a=c  \\\\\n    g(s)              & \\text{if  } a=e\n\\end{cases}\n\\]\nFeature functions \\(\\phi(.)= [\\phi(.)|i=1,\\cdots, m]\\)\n\n\n2.2.2 LSPI Semi-Gradient Equation\n\\[\\sum_i\\phi(s_i).(\\phi(s_i)^T. \\mathbf{w}^*-\\gamma.Q(s_i^{\\prime}, \\pi_D(s_i^{\\prime});\\mathbf{w}^*)\\]\n\n\\[Q(s_i^{\\prime}, \\pi_D(s_i^{\\prime});\\mathbf{w}^*)\\]\n\\(C1\\): if \\(s_i^{\\prime}\\) is non-terminal and \\(\\pi_D(s^{\\prime}) = c(i.e, \\phi(s_i^{\\prime}).\\mathbf{w}>g(s_i^{\\prime}))\\) substitute \\(\\phi(s_i^{\\prime})^T. \\mathbf{w}^*\\)\n\\(C2\\): if \\(s_i^{\\prime}\\) is a terminal state or \\(\\pi_{D}(s_{i}^{\\prime})=e\\) (i.e, \\(g(s_i^{\\prime})>\\phi(s_i^{\\prime}).\\mathbf{w})\\): Substitute \\(g(s_i^{\\prime})\\)\n\n\\[\\sum_i\\phi(s_i).(\\phi(s_i)^T.\\mathbf{w}^*- I_{C1}.\\gamma.\\phi(s_i^{\\prime})^T.\\mathbf{w}^*-I_{C2}.\\gamma.g(s_{i}^{\\prime}))\\]\nFactoring out \\(\\mathbf{w}^{*}\\) we get:\n\\[\\sum_i\\phi(s_i).(\\phi(s_i)- I_{C1}.\\gamma.\\phi(s_i^{\\prime})^T).\\mathbf{w}^*=\\sum_{i} I_{C2}.\\phi(s_i).g(s_{i}^{\\prime}))\\]\nThis can be written in\n\\[\\mathbf{A}.\\mathbf{w}^*=b\\]\n\n\\[\\mathbf{A}=\\sum_i\\phi(s_i).(\\phi(s_i)- I_{C1}.\\gamma.\\phi(s_i^{\\prime})^T)\\]\n\\[\\mathbf{b}=\\sum_{i} I_{C2}.\\phi(s_i).g(s_{i}^{\\prime}))\\]\n\nThe \\(m\\times m\\) Matrix \\(\\mathbf{A}\\) is accumulated at each atomic experince \\((s_i, s_i^{\\prime})\\):\n\n\\[\\mathbf{A} \\Leftarrow \\mathbf{A} + \\phi(s_i).(\\phi(s_i)- I_{C1}.\\gamma.\\phi(s_i^{\\prime})^T)\\]\n\\[\\mathbf{b} \\Leftarrow \\mathbf{b} + \\gamma I_{C2}.\\phi(s_i).g(s_{i}^{\\prime})\\]\n\nThis solved \\(w^{*}\\) updates Q-valueFunction Approximation \\(Q(s,a; \\mathbf{w}^*)\\). This defines an updated , imrpoved deterministic policy \\(\\pi_{D}^{\\prime}\\) :\n\\[\\pi_{D}^{\\prime}(s) = \\operatorname*{arg max }_{a} Q(s,a; \\mathbf{w}^*)\\]\nUsing 7 feature functions, the first 4 Laguerre polyomials that are function of underlying price and 3 functions of time.\n\n\\(\\phi(S_t) = 1\\)\n\\(\\phi(S_t) = e^{-\\frac{M_t}{2}}\\)\n\\(\\phi(S_t) = e^{-\\frac{M_t}{2}}.(1-M_t)\\)\n\\(\\phi(S_t) = e^{-\\frac{M_t}{2}}.(1-2M_t+M_t^2/2)\\)\n\\(\\phi(S_t) = sin(\\frac{\\pi(T-t)}{2T})\\)\n\\(\\phi(S_t) = log(T-t)\\)\n\\(\\phi(S_t) = (\\frac{t}{T})^2\\)"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "See Knuth (1984) for additional discussion of literate programming.\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "problem.html",
    "href": "problem.html",
    "title": "3  Problem Describtion",
    "section": "",
    "text": "The most basic forms of American Options are American Call and Put Options. American Call and Put Options are essentially extensions of the corresponding European Call and Put Options by allowing the buyer (owner) of the American Option to exercise the option to buy (in the case of Call) or sell (in the case of Put) at any time \\(t ≤ T\\) . The allowance of exercise at any time at or before the expiry time \\(T\\) can often be a tricky financial decision for the option owner. At each point in time when the American Option is in-the-money (i.e., positive payoff upon exercise), the option owner might be tempted to exercise and collect the payoff but might as well be thinking that if she waits, the option might become more in-the-money (i.e., prospect of a bigger payoff if she waits for a while). Hence, it’s clear that an American Option is always of the “Option-Type” since the timing of the decision (option) to exercise is very important in the case of an American Option.\nThis also means that the problem of pricing an American Option (the fair price the buyer would need to pay to own an American Option) is much harder than the problem of pricing an European Option. This is a book created from markdown and executable code.\nWe can define Optimal Stopping problem of American Option Problem in Markov Decision Process (MDP):\n\nThe MDP State at time \\(t\\) is \\(X_t\\)\nThe MDP Action is Boolean: Stop the Process or Continue the Process.\nThe MDP Reward is always 0, except upon Stopping, when it is equal to payoff\nThe MDP Discount Factor \\(\\lambda\\) is equal to 1.\nThe MDP probabilistic -transitions are governed by the Stochastic Process \\(X\\)\n\nSee Knuth (1984) for additional discussion of literate programming.\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "prices.html",
    "href": "prices.html",
    "title": "4  Price Models",
    "section": "",
    "text": "Consider a stochastic process \\(x\\) described in the form of the following Ito process:\n\\[dx_t = \\mu(t).x_t.d_t + \\sigma(t).x_t.dz_t\\]\nThe special case of \\(\\mu(t)=\\mu (constant)\\) and \\(\\sigma(t) = \\sigma(constant)\\) known as Geomstric Brownian Motion, to reflect the fact that the stochastic increment of the process \\((\\sigma.x_t.dz_t)\\) is multiplicative to the level of the process \\(x_t\\). If we cosnider this special case, we get:\n\\[y_T = log(x_T) \\sim \\mathcal{N}(log(x_S) + (\\mu-\\frac{\\sigma^2}{2})(T-S), \\sigma^2(T-S))\\]\n\\[E[x_T|x_S]=x_S.e^{\\mu(T-S)}\\]\n\\[Variance[x_T|x_S]=x_S^2.e^{2\\mu(T-S)}.(e^{\\sigma^2(T-S)}-1)\\]"
  },
  {
    "objectID": "results.html",
    "href": "results.html",
    "title": "5  Results",
    "section": "",
    "text": "See Knuth (1984) for additional discussion of literate programming.\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "6  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Paper_manuscript",
    "section": "",
    "text": "This is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books."
  },
  {
    "objectID": "methods.html",
    "href": "methods.html",
    "title": "2  Methods",
    "section": "",
    "text": "The orginal Bin was developed to price option on underlying whose price evolves according to a **lognormal stochastic process, where stochartic process is in form of discrete-time, finite-horizon, finite states.\n\\[dS_t = r.S_t.dt + \\sigma.S_t.dz_t\\]\n\n\\(\\sigma\\) is the lognormal dispersion (volatility), \\(r\\) lognormal drift\n\nWhat is Bin : serves as a discrete-time, finite-horizon, finite-states approximation to the continous proocess.\n\n\n\n\nFigure 2.1: Binomial Option Pricing Model (Binomial Tree)\n\n\n\nWe denote \\(S_{i,j}\\) as the price after \\(i\\) time steps in state \\(j\\) (for any \\(i\\) Z>0 and for any \\(0<j<i\\)),\nWe need to calibrate \\(S_{i+1,j+1}=S_{i,j}.u\\) and \\(S_{i+1,j}=S_{i,j}.d\\)\nVariance Calibration\n\\[log^²(u) = \\frac{\\sigma^2T}{n} \\Rightarrow u = e^{\\sigma \\sqrt{\\frac{T}{n}} }\\]\nMean Calibration\n\\[qu + \\frac{1-q}{u} = e^{\\frac{rT}{n}} \\Rightarrow q = \\frac{u.e^{\\frac{rT}{n}-}-1}{u^2-1}=\\frac{e^{\\frac{rT}{n}+\\sigma\\sqrt{\\frac{T}{n}}}-1}{e^{2\\sigma\\sqrt{{\\frac{T}{n}}}}-1}\\] ## Least Square Monte-Carlo\nThe calibration for \\(u\\) and \\(q\\) ensures that as \\(n \\rightarrow \\infty\\) (i.e., time step interval \\(\\frac{T}{n} \\rightarrow 0\\)), the mean and variance of binomial distributaion after i time steps matches the mean \\((r-\\frac{\\sigma^2}{2})\\frac{iT}{n}\\) and variance \\(\\frac{\\sigma^2iT}{n}\\)\n\n\n\nAs mentioned earlier, we want to model the problem of Optimal Exercise of American Options as a discrete-time, finite-horizon, finite-states MDP. We set the terminal time to be \\(t = T + 1\\), meaning all the states at time \\(T + 1\\) are terminal states. Here we will utilize the states and state transitions (probabilistic price movements of the underlying) given by the Binomial Options Pricing Model as the states and state transitions in the MDP. The MDP actions in each state will be binary—either exercise the option (and immediately move to a terminal state) or don’t exercise the option (i.e., continue on to the next time step’s random state, as given by the Binomial Options Pricing Model). If the exercise action is chosen, the MDP reward is the option payoff. If the continue action is chosen, the reward is \\(0\\). The discount factor \\(\\lambda\\) is \\(e^{-\\frac{rT}{n}}\\) since (as we’ve learnt in the single-period case), the price (which translates here to the Optimal Value Function) is defined as the riskless ratediscounted expectation (under the risk-neutral probability measure) of the option payoff. In the multi-period setting, the overall discounting amounts to composition (multiplication) of each time step’s discounting (which is equal to \\(\\lambda\\)) and the overall risk-neutral probability measure amounts to the composition of each time step’s risk-neutral probability measure (which is specified by the calibrated value \\(q\\))."
  },
  {
    "objectID": "methods.html#lsm-method",
    "href": "methods.html#lsm-method",
    "title": "2  Methods",
    "section": "2.3 LSM Method",
    "text": "2.3 LSM Method\n\n2.3.1 The Pricing is based on set of sampling traces of the underlying prices.\n\n\n2.3.2 Function approximation of the continuation value for in-the-money states.\nThe LSM approach uses least squares to approximate the conditional expectaion function at \\(t_{K-1}, t_{K-2}, \\cdots, t_1\\):\n\nUnknown functional form:\n\n\\[F(\\omega; t_{K-1})\\]\n\nThe unknown functional form can be represented as:\n\n\\[F(\\omega; t_{K-1})=\\sum_{j=0}^{\\infty}a_jL_j(X)\\]\nIn this work choice of basis function is set of Lageurre polynomials:\n\\[L_1(X)=1\\]\n\\[L_2(X)= 1-X\\]\n\\[L_3(X) = 1 - 2X + X^2/2\\]\n\\[L_4(X) = 1 - 3X + 3X^2/2 - X^3/6\\]\n\\[L_5(X) = 1 - 4X + 3X^2 - 2X^3/3 + X^4/24\\]\n\nIn the python code unknown function can be represented as:\n\n\\[F(\\omega; t_{K-1})=\\sum_{j=0}^{5}a_jL_j(X)\\]\nIn this work we focus only in-the-money paths in the estimation since the exercise decision is only relevant when the option is in the money.\n\n\n2.3.3 Backward -recursive determination of early exercise states"
  },
  {
    "objectID": "methods.html#klsm-method",
    "href": "methods.html#klsm-method",
    "title": "2  Methods",
    "section": "2.3 kLSM Method",
    "text": "2.3 kLSM Method\n\n2.3.1 The Pricing is based on set of sampling traces of the underlying prices.\n\n\n2.3.2 Function approximation of the continuation value for in-the-money states.\nThe LSM approach uses least squares to approximate the conditional expectaion function at \\(t_{K-1}, t_{K-2}, \\cdots, t_1\\):\n\nUnknown functional form:\n\n\\[F(\\omega; t_{K-1})\\]\n\nThe unknown functional form can be represented as:\n\n\\[F(\\omega; t_{K-1})=\\sum_{j=0}^{\\infty}a_jL_j(X)\\]\nIn this work choice of basis function is set of Lageurre polynomials:\n\\[L_1(X)=1\\]\n\\[L_2(X)= 1-X\\]\n\\[L_3(X) = 1 - 2X + X^2/2\\]\n\\[L_4(X) = 1 - 3X + 3X^2/2 - X^3/6\\]\n\\[L_5(X) = 1 - 4X + 3X^2 - 2X^3/3 + X^4/24\\]\n\nIn the python code unknown function can be represented as:\n\n\\[F(\\omega; t_{K-1})=\\sum_{j=0}^{5}a_jL_j(X)\\]\nIn this work we focus only in-the-money paths in the estimation since the exercise decision is only relevant when the option is in the money.\n\n\n2.3.3 Backward -recursive determination of early exercise states"
  },
  {
    "objectID": "prices.html#the-garch-model",
    "href": "prices.html#the-garch-model",
    "title": "4  Price Models",
    "section": "4.2 The GARCH model",
    "text": "4.2 The GARCH model\nGeneralized Autoregressive Conditional Heteroskedasticity (GARCH) process is defined as\n\\[r_t = \\mu_t + \\epsilon_t\\]\nwhere \\(r_t\\) is…, where \\(\\mu_t\\) can be any adapted model for the conditional mean. \\(\\epsilon_t\\) can be defined as :\n\\[\\sigma_t^2 = \\omega + \\sum_{p=1}^{P}\\alpha_p\\epsilon_{t-p}^{2} + \\sum_{q=1}^{Q}\\beta_q\\sigma_{t-q}^{2}\\]\n\\[\\epsilon_t = \\sigma_te_t\\]\n\\[e_t \\sim N(0,1)\\]\nA simple GARCH (1,1) can be defined as:\n\\[r_t= \\mu_t + \\epsilon_t\\]\n\\[\\sigma_t = \\omega + \\alpha_1\\epsilon_{t-1}^² + \\beta\\sigma_{t-1}^2\\]\n\\[\\epsilon_t = \\sigma_te_t\\]\n\\[e_t \\sim N(0,1)\\]\n\n4.2.1 Maximum Likelihood\nSince the errors are assumed to be conditionally i.i.d normal, maximum likelihood is a natural choice to estimate the unknown parameters, \\(\\theta\\) which contain both the mean and variance parameters. The normal likelihood for \\(T\\) independent variables is:\n\\[f(r;\\theta) = (2\\pi\\sigma_t^{2})^{-1/2}exp(-\\frac{-(r_t-\\mu_t)}{2\\sigma_t^2})\\]\nAnd the normal log-likelihood function is:\n\\[l(\\mathbf{r};\\theta)= \\sum_{t=1}^{T}\\log(f(r;\\theta)) = \\sum_{t=1}^{T}-\\frac{1}{2}\\log(2\\pi)-\\frac{1}{2}\\log(\\sigma_t^2)-\\frac{(r_t-\\mu)}{2\\sigma_{t}^2}\\]\nhttps://excalidraw.com/#room=7cb6f12f634b77bc46df,VM1vTjmFl12La60pEZuUKg\n\n\n\nFigure 4.1: Process of Generating New Path using GARCH(1,1) Model\n\n\nThis is a b isook created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  }
]