[
  {
    "objectID": "methods.html#least-square-monte-carlo",
    "href": "methods.html#least-square-monte-carlo",
    "title": "2  Methods",
    "section": "2.2 Least Square Monte-Carlo",
    "text": "2.2 Least Square Monte-Carlo"
  },
  {
    "objectID": "methods.html#reinforcement-learning",
    "href": "methods.html#reinforcement-learning",
    "title": "2  Methods",
    "section": "2.2 Reinforcement Learning",
    "text": "2.2 Reinforcement Learning"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "See Knuth (1984) for additional discussion of literate programming.\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "problem.html",
    "href": "problem.html",
    "title": "3  Problem Describtion",
    "section": "",
    "text": "See Knuth (1984) for additional discussion of literate programming.\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "prices.html",
    "href": "prices.html",
    "title": "4  Price Models",
    "section": "",
    "text": "See Knuth (1984) for additional discussion of literate programming.\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "results.html",
    "href": "results.html",
    "title": "5  Results",
    "section": "",
    "text": "See Knuth (1984) for additional discussion of literate programming.\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "6  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Paper_manuscript",
    "section": "",
    "text": "This is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books."
  },
  {
    "objectID": "methods.html",
    "href": "methods.html",
    "title": "2  Methods",
    "section": "",
    "text": "The orginal Bin was developed to price option on underlying whose price evolves according to a **lognormal stochastic process, where stochartic process is in form of discrete-time, finite-horizon, finite states.\n\\[dS_t = r.S_t.dt + \\sigma.S_t.dz_t\\]\n\n\\(\\sigma\\) is the lognormal dispersion (volatility), \\(r\\) lognormal drift\n\nWhat is Bin : serves as a discrete-time, finite-horizon, finite-states approximation to the continous proocess.\n\n\n\n\nFigure 2.1: Binomial Option Pricing Model (Binomial Tree)\n\n\n\nWe denote \\(S_{i,j}\\) as the price after \\(i\\) time steps in state \\(j\\) (for any \\(i\\) Z>0 and for any \\(0<j<i\\)),\nWe need to calibrate \\(S_{i+1,j+1}=S_{i,j}.u\\) and \\(S_{i+1,j}=S_{i,j}.d\\)\nVariance Calibration\n\\[log^²(u) = \\frac{\\sigma^2T}{n} \\Rightarrow u = e^{\\sigma \\sqrt{\\frac{T}{n}} }\\]\nMean Calibration\n\\[qu + \\frac{1-q}{u} = e^{\\frac{rT}{n}} \\Rightarrow q = \\frac{u.e^{\\frac{rT}{n}-}-1}{u^2-1}=\\frac{e^{\\frac{rT}{n}+\\sigma\\sqrt{\\frac{T}{n}}}-1}{e^{2\\sigma\\sqrt{{\\frac{T}{n}}}}-1}\\] ## Least Square Monte-Carlo\nThe calibration for \\(u\\) and \\(q\\) ensures that as \\(n \\rightarrow \\infty\\) (i.e., time step interval \\(\\frac{T}{n} \\rightarrow 0\\)), the mean and variance of binomial distributaion after i time steps matches the mean \\((r-\\frac{\\sigma^2}{2})\\frac{iT}{n}\\) and variance \\(\\frac{\\sigma^2iT}{n}\\)\n\n\n\nAs mentioned earlier, we want to model the problem of Optimal Exercise of American Options as a discrete-time, finite-horizon, finite-states MDP. We set the terminal time to be \\(t = T + 1\\), meaning all the states at time \\(T + 1\\) are terminal states. Here we will utilize the states and state transitions (probabilistic price movements of the underlying) given by the Binomial Options Pricing Model as the states and state transitions in the MDP. The MDP actions in each state will be binary—either exercise the option (and immediately move to a terminal state) or don’t exercise the option (i.e., continue on to the next time step’s random state, as given by the Binomial Options Pricing Model). If the exercise action is chosen, the MDP reward is the option payoff. If the continue action is chosen, the reward is \\(0\\). The discount factor \\(\\lambda\\) is \\(e^{-\\frac{rT}{n}}\\) since (as we’ve learnt in the single-period case), the price (which translates here to the Optimal Value Function) is defined as the riskless ratediscounted expectation (under the risk-neutral probability measure) of the option payoff. In the multi-period setting, the overall discounting amounts to composition (multiplication) of each time step’s discounting (which is equal to γ) and the overall risk-neutral probability measure amounts to the composition of each time step’s risk-neutral probability measure (which is specified by the calibrated value q)."
  }
]